# CNN and Transformer cross–teaching for multimodal image cell segmentation #

The ’Weakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Images’ competition was organized by Neural Information Processing
Systems (NeurIPS) to challenge the participants to find a cell segmentation methods that could be applied to various microscopy images across multiple 
imaging platforms and tissue types. The goal is to create a generic, reusable model that is trained once and can be reused on a variety of different 
microscopy experiments without further user intervention. 

Our solution is based on a cross-teaching between a Convolutional Neural Network (CNN) and a Transformer. 
This framework takes both labeled and unlabeled images as inputs, and each image passes a CNN and a Transformer respectively to produce the prediction. 
For the labeled data, the CNN and Transformer are supervised by the ground truth individually. Then the predictions of unlabeled images generated by 
CNN/Transformer are used to update the parameters of the Transformer/CNN respectively. This framework benefits from the two different learning paradigms,
CNNs focus on the local information and transformers model the long range relation, so the cross teaching can help to learn a unified segmenter with these 
two properties at the same time.

## Description:

- preprocessing.py:
    Image pre-processing: the number of channels are set to three, repeting the images with one channel; 
                          then the images are normalized (intensity normalization) and saved as uint16
    Mask pre-processing:  the instance segmentation masks are transformed into masks with three classes 
                          (background, interior and boundary) and then saved as uint16
    The pre-process dataset is saved in the Train_Pre_3class folder

- Add the script for the distance and neighbour lables 
    
- train.py:
    Baseline training: U-Net, ViT+U-Net (unetr2d.py), Swin Transformer + U-Net. The three models are created with MONAI framework.
    The images are cropped with a 256x256 dimension and augmented with MONAI. The model is saved in the work_dir folder.

- train_bal_val_ctc.py: 
    Cross-teaching between U-Net and the Swin Transformer + U-Net. The two models are created with MONAI framework.
    The images are cropped with a 256x256 dimension and augmented with MONAI.
    The model is saved in the a folder (name need to be specified).

- unetr2d.py: 
    This script creates the ViT + U-Net model using MONAI (one of the baseline models proposed by NeurIPS)

- predict_5class.py: 
    prediction using the U-net or the Swin Transformer + U-Net

- example_images: 
    The 'image' folder has some example of images used during the NeurIPS challenge. On the 'label' folder there are the the lables corresponding 
    to the images.

